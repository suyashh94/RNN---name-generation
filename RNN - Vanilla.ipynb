{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "There are 19909 total characters and 27 unique characters in your data.\n"
     ]
    }
   ],
   "source": [
    "data = open(r'C:\\Users\\harla\\Desktop\\Jupyter Notebooks\\RNN basic\\dinos.txt', 'r').read()\n",
    "data= data.lower()\n",
    "chars = list(set(data))\n",
    "print(sorted(chars))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n",
      "Total charachters are : 27\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "#ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
    "ix_to_char = {}\n",
    "for i,ch in enumerate(sorted(chars)):\n",
    "    ix_to_char[(i)] = ch\n",
    "print(ix_to_char)\n",
    "print(\"Total charachters are :\", len(ix_to_char))\n",
    "#print(type(char_to_ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(loss, cur_loss):\n",
    "    return loss * 0.999 + cur_loss * 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_a, n_x, n_y):\n",
    "    \n",
    "    Wax = np.random.randn(n_a, n_x)*0.01 # input to hidden\n",
    "    Waa = np.random.randn(n_a, n_a)*0.01 # hidden to hidden\n",
    "    Wya = np.random.randn(n_y, n_a)*0.01 # hidden to output\n",
    "    b = np.zeros((n_a, 1)) # hidden bias\n",
    "    by = np.zeros((n_y, 1)) # output bias\n",
    "    \n",
    "    parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b,\"by\": by}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_loss(vocab_size, seq_length):\n",
    "    return -np.log(1.0/vocab_size)*seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_step_forward(parameters, a_prev, x):\n",
    "    \n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    a_next = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b) # hidden state\n",
    "    p_t = softmax(np.dot(Wya, a_next) + by) # unnormalized log probabilities for next chars # probabilities for next chars \n",
    "    \n",
    "    return a_next, p_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward(X, Y, a0, parameters, vocab_size = 27):\n",
    "    \n",
    "    x, a, y_hat = {}, {}, {}\n",
    "    \n",
    "    a[-1] = np.copy(a0)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    for t in range(len(X)):\n",
    "        \n",
    "        x[t] = np.zeros((vocab_size,1)) \n",
    "        if (X[t] != None):\n",
    "            x[t][X[t]] = 1\n",
    "        \n",
    "        a[t], y_hat[t] = rnn_step_forward(parameters, a[t-1], x[t])\n",
    "        \n",
    "        loss -= np.log(y_hat[t][Y[t],0])\n",
    "        \n",
    "    cache = (y_hat, a, x)\n",
    "        \n",
    "    return loss, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_step_backward(dy, gradients, parameters, x, a, a_prev):\n",
    "    \n",
    "    gradients['dWya'] += np.dot(dy, a.T)\n",
    "    gradients['dby'] += dy\n",
    "    da = np.dot(parameters['Wya'].T, dy) + gradients['da_next'] # backprop into h\n",
    "    daraw = (1 - a * a) * da # backprop through tanh nonlinearity\n",
    "    gradients['db'] += daraw\n",
    "    gradients['dWax'] += np.dot(daraw, x.T)\n",
    "    gradients['dWaa'] += np.dot(daraw, a_prev.T)\n",
    "    gradients['da_next'] = np.dot(parameters['Waa'].T, daraw)\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_backward(X, Y, parameters, cache):\n",
    "    gradients = {}\n",
    "    \n",
    "    (y_hat, a, x) = cache\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    \n",
    "    gradients['dWax'], gradients['dWaa'], gradients['dWya'] = np.zeros_like(Wax), np.zeros_like(Waa), np.zeros_like(Wya)\n",
    "    gradients['db'], gradients['dby'] = np.zeros_like(b), np.zeros_like(by)\n",
    "    gradients['da_next'] = np.zeros_like(a[0])\n",
    "    \n",
    "    \n",
    "    for t in reversed(range(len(X))):\n",
    "        dy = np.copy(y_hat[t])\n",
    "        dy[Y[t]] -= 1\n",
    "        gradients = rnn_step_backward(dy, gradients, parameters, x[t], a[t], a[t-1])\n",
    "    \n",
    "    return gradients, a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, lr):\n",
    "\n",
    "    parameters['Wax'] += -lr * gradients['dWax']\n",
    "    parameters['Waa'] += -lr * gradients['dWaa']\n",
    "    parameters['Wya'] += -lr * gradients['dWya']\n",
    "    parameters['b']  += -lr * gradients['db']\n",
    "    parameters['by']  += -lr * gradients['dby']\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(gradients, maxValue):\n",
    "    \n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
    "   \n",
    "    for gradient in [dWax, dWaa, dWya, db, dby]:\n",
    "        np.clip(gradient, -1 * maxValue, maxValue, out = gradient)\n",
    "    \n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n",
    "    \n",
    "    loss, cache = rnn_forward(X, Y, a_prev, parameters)\n",
    "    \n",
    "    gradients, a = rnn_backward(X, Y, parameters, cache)\n",
    "    \n",
    "    gradients = clip(gradients, maxValue = 5)\n",
    "    \n",
    "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "    \n",
    "    \n",
    "    return loss, gradients, a[len(X)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(parameters, char_to_ix):\n",
    "    \n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    vocab_size = by.shape[0]\n",
    "    n_a = Waa.shape[1]\n",
    "    \n",
    "    x = np.zeros(shape = (vocab_size,1))\n",
    "    a_prev = np.zeros(shape = (n_a,1))\n",
    "    \n",
    "    indices = []\n",
    "    \n",
    "    idx = -1 \n",
    "    \n",
    "   \n",
    "    counter = 0\n",
    "    newline_character = char_to_ix['\\n']\n",
    "    \n",
    "    while (idx != newline_character and counter != 50):\n",
    "        \n",
    "        a = np.tanh(np.dot(Wax,x) + np.dot(Waa,a_prev) + b)\n",
    "        z = np.dot(Wya,a) + by\n",
    "        y = softmax(z)\n",
    "        \n",
    "        \n",
    "        idx = np.random.choice(list(range(vocab_size)),  p = y.ravel())\n",
    "\n",
    "        indices.append(idx)\n",
    "        \n",
    "        x = np.zeros(shape = (vocab_size,1))\n",
    "        x[idx] = 1\n",
    "        \n",
    "        a_prev = a\n",
    "        \n",
    "    if (counter == 50):\n",
    "        indices.append(char_to_ix['\\n'])\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample(sample_ix, ix_to_char):\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    txt = txt[0].upper() + txt[1:]  # capitalize first character \n",
    "    print ('%s' % (txt, ), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data, ix_to_char, char_to_ix, num_iterations = 35000, n_a = 50, dino_names = 7, vocab_size = 27):\n",
    "    \n",
    "    n_x, n_y = vocab_size, vocab_size\n",
    "    \n",
    "    parameters = initialize_parameters(n_a, n_x, n_y)\n",
    "    \n",
    "    loss = get_initial_loss(vocab_size, dino_names)\n",
    "    \n",
    "    with open(r'C:\\Users\\harla\\Desktop\\Jupyter Notebooks\\RNN basic\\dinos.txt') as f:\n",
    "        examples = f.readlines()\n",
    "    examples = [x.lower().strip() for x in examples]\n",
    "    \n",
    "    np.random.shuffle(examples)\n",
    "    \n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    for j in range(num_iterations):\n",
    "        \n",
    "        index = j % len(examples)\n",
    "        X = [None] + [char_to_ix[ch] for ch in examples[index]]\n",
    "        #print(examples[index])\n",
    "        #print(X)\n",
    "        Y = X[1:] + [char_to_ix[\"\\n\"]]\n",
    "        \n",
    "        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)\n",
    "        \n",
    "        loss = smooth(loss, curr_loss)\n",
    "\n",
    "        if j %  500 == 0:\n",
    "            \n",
    "            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
    "            \n",
    "            for name in range(dino_names):\n",
    "                \n",
    "                sampled_indices = sample(parameters, char_to_ix)\n",
    "                print_sample(sampled_indices, ix_to_char)\n",
    "      \n",
    "            print('\\n')\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss: 23.084039\n",
      "\n",
      "\n",
      "Duswkmyhfavhehbomhmsshibhsiygn\n",
      "Udakpikuktonvjrrqjhywfoheazlooihhrmvrzaahztzbqd\n",
      "Crcrsuljvhwxyjepygveviyrjssuwnphopffntkuxesbiwdrdyrgh\n",
      "Fxocwzceh\n",
      "Yzgqluorzhronaagevoxvnut\n",
      "Zezpzsuiwgbmyifveoohkti\n",
      "\n",
      "\n",
      "Iteration: 500, Loss: 28.062455\n",
      "\n",
      "Hlorurusaxrurus\n",
      "\n",
      "\n",
      "\n",
      "Ror\n",
      "Xhbocnupaoreaaratagruras\n",
      "Zmo\n",
      "\n",
      "\n",
      "Iteration: 1000, Loss: 28.907741\n",
      "\n",
      "Zzmohgsaurus\n",
      "Es\n",
      "Us\n",
      "Thosapillnjxurusairteprasaerur\n",
      "Lhonicaaonil\n",
      "Paurosasaurus\n",
      "Miptvuaurus\n",
      "\n",
      "\n",
      "Iteration: 1500, Loss: 28.684192\n",
      "\n",
      "\n",
      "Ratrus\n",
      "A\n",
      "Blareoporodanhirus\n",
      "Aus\n",
      "Ycyus\n",
      "Irumohus\n",
      "\n",
      "\n",
      "Iteration: 2000, Loss: 27.842364\n",
      "\n",
      "Dotosaurus\n",
      "Annorourus\n",
      "Sauritosaurus\n",
      "Ryalthennbresansaurus\n",
      "Rusnprasaurus\n",
      "Tamipsaurus\n",
      "A\n",
      "\n",
      "\n",
      "Iteration: 2500, Loss: 27.324245\n",
      "\n",
      "Ocerbelon\n",
      "Aleromosaurus\n",
      "Iadtodicochucoga\n",
      "Tonourus\n",
      "Yrueragonhaugusterpa\n",
      "Tonbocrohotoschhabamosaurus\n",
      "Autosaurus\n",
      "\n",
      "\n",
      "Iteration: 3000, Loss: 26.794676\n",
      "\n",
      "Chutolitganytanchus\n",
      "Engataulinongakmodlunophisaurus\n",
      "Rdenmasaurus\n",
      "Antorasaurus\n",
      "Anta\n",
      "Ciceng\n",
      "Andorattansaurus\n",
      "\n",
      "\n",
      "Iteration: 3500, Loss: 26.187059\n",
      "\n",
      "Aadosaurpancoryteroreansaurus\n",
      "Hosaurus\n",
      "Belonosaurum\n",
      "Elonoptaroralorsaurus\n",
      "Lahxaroryophutosaurus\n",
      "Ontausauauscrocheocuenanalelalus\n",
      "Auvopodhercorhvosaurus\n",
      "\n",
      "\n",
      "Iteration: 4000, Loss: 25.852692\n",
      "\n",
      "Hunocara\n",
      "Apindicanodasaurus\n",
      "Glenis\n",
      "Sitarcholorataur\n",
      "Tosaurus\n",
      "Suskiaboxicaichuhuanosauris\n",
      "Samiriatorizurosaurus\n",
      "\n",
      "\n",
      "Iteration: 4500, Loss: 25.594442\n",
      "\n",
      "Pvomaisaurus\n",
      "Sauroptosaurus\n",
      "Ngcertayatocelgaxuwph\n",
      "Angidecyiaptanabrasaurus\n",
      "Hetytalia\n",
      "Dalia\n",
      "Tomesaurus\n",
      "\n",
      "\n",
      "Iteration: 5000, Loss: 25.171209\n",
      "\n",
      "Ssaudosaurus\n",
      "Annux\n",
      "Yuskiaphetotisaurus\n",
      "Teifocir\n",
      "Saunasaurus\n",
      "Taurus\n",
      "Osaurus\n",
      "\n",
      "\n",
      "Iteration: 5500, Loss: 24.951481\n",
      "\n",
      "Opmornahyodytumus\n",
      "Eosaubus\n",
      "Istoyngesaurus\n",
      "Kancorlkereploborus\n",
      "Njtengongsaurus\n",
      "Reenptorosauras\n",
      "Hkonuzeyculus\n",
      "\n",
      "\n",
      "Iteration: 6000, Loss: 24.861712\n",
      "\n",
      "Richerasaurus\n",
      "Saurus\n",
      "Saurucurataus\n",
      "Erracipanin\n",
      "Cascrosaurus\n",
      "Racirsheshispalelichoasaurus\n",
      "Aialia\n",
      "\n",
      "\n",
      "Iteration: 6500, Loss: 24.546711\n",
      "\n",
      "Saurosaurus\n",
      "Elyusnitedanisaurus\n",
      "Kerovondplea\n",
      "Libiraltandelylisaurus\n",
      "Phidrhalatimos\n",
      "Hudorriinrishenanoengitonvakosploregthovheltosaurus\n",
      "Talox\n",
      "\n",
      "\n",
      "Iteration: 7000, Loss: 24.435813\n",
      "\n",
      "Sauautag\n",
      "Adiaspaecaramamchyhaposaurus\n",
      "Augrioi\n",
      "Hinouanmjiaupanavanghehesaurus\n",
      "Yalia\n",
      "Enamilannisaurus\n",
      "Sthosaurus\n",
      "\n",
      "\n",
      "Iteration: 7500, Loss: 24.369793\n",
      "\n",
      "Werochosaurus\n",
      "Auodebanyphishunopanstabinia\n",
      "Toror\n",
      "Endos\n",
      "Elalocrous\n",
      "Lomantasaurus\n",
      "Albisaurus\n",
      "\n",
      "\n",
      "Iteration: 8000, Loss: 24.076493\n",
      "\n",
      "Cikozeor\n",
      "Rites\n",
      "Ratodiratibis\n",
      "Orhyror\n",
      "Acharus\n",
      "Achaeran\n",
      "Cheskomesaurus\n",
      "\n",
      "\n",
      "Iteration: 8500, Loss: 24.063006\n",
      "\n",
      "Pichasingeuzcodtoosaurus\n",
      "Prodinvhiong\n",
      "Tadannjinypinxixipgosaurus\n",
      "Raazeoonnsseryioimrienosaurus\n",
      "Siinotaosaurus\n",
      "Kinrcdus\n",
      "Pikovetrosaurus\n",
      "\n",
      "\n",
      "Iteration: 9000, Loss: 24.017093\n",
      "\n",
      "Gntmogoxavosaurus\n",
      "Eysaurus\n",
      "Lyusodontopeotiiaptinus\n",
      "Chobeteralocpiloperttopesaurus\n",
      "Rapensaurus\n",
      "Osous\n",
      "Ilosaurus\n",
      "\n",
      "\n",
      "Iteration: 9500, Loss: 23.841115\n",
      "\n",
      "Pasthoa\n",
      "Siderodacsus\n",
      "Upalirosaurus\n",
      "Helosous\n",
      "Osaurus\n",
      "Arhlylus\n",
      "Heraraegopton\n",
      "\n",
      "\n",
      "Iteration: 10000, Loss: 23.711004\n",
      "\n",
      "Oshemosaurus\n",
      "Lodongophus\n",
      "Uwvyolkosaurus\n",
      "Erodosaurus\n",
      "Ucaus\n",
      "Lipopintosaurus\n",
      "Raicorosaurus\n",
      "\n",
      "\n",
      "Iteration: 10500, Loss: 23.773276\n",
      "\n",
      "Styhus\n",
      "Duryasdesatovg\n",
      "Stangximonsaurus\n",
      "Velitondos\n",
      "Esaprus\n",
      "Xonisaurus\n",
      "Osaurus\n",
      "\n",
      "\n",
      "Iteration: 11000, Loss: 23.607299\n",
      "\n",
      "Rusapotophipmodenogoriosaurus\n",
      "Anneia\n",
      "Liygodon\n",
      "Eidrwangonabbosausus\n",
      "Sacoploton\n",
      "Gantontevodeoeengtes\n",
      "Nuptoridr\n",
      "\n",
      "\n",
      "Iteration: 11500, Loss: 23.506417\n",
      "\n",
      "Riosaurus\n",
      "Yrosaurus\n",
      "Isitois\n",
      "Anianiasaurus\n",
      "Osaurus\n",
      "Hydipus\n",
      "Liagnasaurus\n",
      "\n",
      "\n",
      "Iteration: 12000, Loss: 23.541547\n",
      "\n",
      "Atgasaurus\n",
      "Alahaloces\n",
      "Tiengchisaurus\n",
      "Ticuralingnyron\n",
      "Atasaurus\n",
      "Liisiuan\n",
      "Animososaurus\n",
      "\n",
      "\n",
      "Iteration: 12500, Loss: 23.431561\n",
      "\n",
      "Butopausaurus\n",
      "Eabelchaasaurus\n",
      "Hesaurus\n",
      "Aphakegatrus\n",
      "Antalalopausus\n",
      "Uthaporpoldolthasaurus\n",
      "Ocerapojraeroplosaurus\n",
      "\n",
      "\n",
      "Iteration: 13000, Loss: 23.312431\n",
      "\n",
      "Sicangus\n",
      "Resteotognatosaurus\n",
      "Anatotgnyx\n",
      "Lochusaurus\n",
      "Langusaurus\n",
      "Charosaurus\n",
      "Saus\n",
      "\n",
      "\n",
      "Iteration: 13500, Loss: 23.419531\n",
      "\n",
      "Hoenisopinasaurus\n",
      "Aipilomesaurus\n",
      "Anterys\n",
      "Sknhuchis\n",
      "Aloden\n",
      "Phsuiamolopessanparops\n",
      "Hitiaisaurus\n",
      "\n",
      "\n",
      "Iteration: 14000, Loss: 23.324584\n",
      "\n",
      "Santhuannasaurus\n",
      "Lusinusaurus\n",
      "Teltyoponle\n",
      "Adanodon\n",
      "Ikyparosaurus\n",
      "Chylonkalpstenonninopamix\n",
      "Cous\n",
      "\n",
      "\n",
      "Iteration: 14500, Loss: 23.222757\n",
      "\n",
      "Inthonis\n",
      "Noljenitakosaurus\n",
      "Harmdotepitos\n",
      "Segawteraptorus\n",
      "Trajotasaurus\n",
      "Anttotachus\n",
      "Admannaorolus\n",
      "\n",
      "\n",
      "Iteration: 15000, Loss: 23.293328\n",
      "\n",
      "Huthosphops\n",
      "Hucloperaptor\n",
      "Usaurus\n",
      "Inthisaurus\n",
      "Volosaurus\n",
      "Ankasukhyyewlosphoptosaurus\n",
      "Xaedaleosaurus\n",
      "\n",
      "\n",
      "Iteration: 15500, Loss: 23.255718\n",
      "\n",
      "Osaa\n",
      "Alatoperatops\n",
      "Lotorodon\n",
      "Istematops\n",
      "Ctimusaurus\n",
      "Hemophagorsptus\n",
      "Gratusuphacosaurus\n",
      "\n",
      "\n",
      "Iteration: 16000, Loss: 23.038271\n",
      "\n",
      "Averatops\n",
      "Alglosaurus\n",
      "Ingosaurus\n",
      "Alycolantopsaurithasaurus\n",
      "Canaandia\n",
      "Cleinodephus\n",
      "Clorastitethrapterus\n",
      "\n",
      "\n",
      "Iteration: 16500, Loss: 23.246153\n",
      "\n",
      "Tirnanoshux\n",
      "Toealoceraplodr\n",
      "Monlceraotas\n",
      "Anasaurus\n",
      "Lophaceratoptosthurosaurus\n",
      "Starosaurus\n",
      "Oplagtyerawasaurus\n",
      "\n",
      "\n",
      "Iteration: 17000, Loss: 23.150585\n",
      "\n",
      "Chanotia\n",
      "Ricenotrrus\n",
      "Arlibelljengrinong\n",
      "Byloniodon\n",
      "Iacorithus\n",
      "Lasuroog\n",
      "Lterateosaerus\n",
      "\n",
      "\n",
      "Iteration: 17500, Loss: 22.998405\n",
      "\n",
      "Huniorcius\n",
      "Patomalesaurus\n",
      "Corbistosaurus\n",
      "Husthosaurus\n",
      "Rupocorespa\n",
      "Chihosaurus\n",
      "Nodacoriucucerossaurus\n",
      "\n",
      "\n",
      "Iteration: 18000, Loss: 23.140162\n",
      "\n",
      "Ocerlokesaurus\n",
      "Hthisaurus\n",
      "Dingurgodon\n",
      "Araptor\n",
      "Nomisaurus\n",
      "Us\n",
      "Qhasaurulutax\n",
      "\n",
      "\n",
      "Iteration: 18500, Loss: 23.192729\n",
      "\n",
      "Rakhodrodon\n",
      "Onihtosaurus\n",
      "Tothosaurus\n",
      "Xualoden\n",
      "Lopologn\n",
      "Laceptor\n",
      "Xulivus\n",
      "\n",
      "\n",
      "Iteration: 19000, Loss: 22.921765\n",
      "\n",
      "Ruroudinesaurus\n",
      "Osaurus\n",
      "Urona\n",
      "Antosaurus\n",
      "Osaurus\n",
      "Hiimosaurus\n",
      "Aranarna\n",
      "\n",
      "\n",
      "Iteration: 19500, Loss: 23.127239\n",
      "\n",
      "Cesvondannkosauruc\n",
      "Leeraptor\n",
      "Aldodrator\n",
      "Omelosaurus\n",
      "Stinoraunzyaltor\n",
      "Koraptor\n",
      "Chylorus\n",
      "\n",
      "\n",
      "Iteration: 20000, Loss: 23.076071\n",
      "\n",
      "Pusousosaurus\n",
      "Ruanucurus\n",
      "Robisaurus\n",
      "Honirus\n",
      "Sacemosuc\n",
      "Bepctotrosaurus\n",
      "Siankosaurus\n",
      "\n",
      "\n",
      "Iteration: 20500, Loss: 22.812366\n",
      "\n",
      "Corylole\n",
      "Lmcrogcera\n",
      "Gnotikmonnosur\n",
      "Huszleps\n",
      "Sbtetyus\n",
      "Luroclordinonnchus\n",
      "Etenkodonhodunondosaurus\n",
      "\n",
      "\n",
      "Iteration: 21000, Loss: 22.983959\n",
      "\n",
      "Akrichuctirala\n",
      "Hizinosaurus\n",
      "Inanshusinorolus\n",
      "Ngraityps\n",
      "Husafatatyirosaurus\n",
      "Losatrustlatasaurus\n",
      "Hocadroptor\n",
      "\n",
      "\n",
      "Iteration: 21500, Loss: 22.910892\n",
      "\n",
      "Acroves\n",
      "Daninachumaxranaheitisaurus\n",
      "Xisaus\n",
      "Andogphaus\n",
      "Arazlotor\n",
      "Cerrosaurus\n",
      "Resosaurus\n",
      "\n",
      "\n",
      "Iteration: 22000, Loss: 22.701281\n",
      "\n",
      "Riptoryphidnleurape\n",
      "Craptor\n",
      "Luspprospen\n",
      "Leylosaurus\n",
      "Hitrosaurus\n",
      "Suaponom\n",
      "Romanseosaurus\n",
      "\n",
      "\n",
      "Iteration: 22500, Loss: 22.920222\n",
      "\n",
      "Hurnitisucrus\n",
      "Ertorufeeltsaurus\n",
      "Heosifasaurus\n",
      "Strychintaun\n",
      "Inayanastenitlongosacraphnenocophamnarosaurus\n",
      "Hyisinolondryisivecalinuchichunosatrus\n",
      "Ngyrhunosaurus\n",
      "\n",
      "\n",
      "Iteration: 23000, Loss: 22.906677\n",
      "\n",
      "Elmenosheiosaurus\n",
      "Mistitateratofdomel\n",
      "Syuairacons\n",
      "Yutkhatosaurodlus\n",
      "Altonsaurus\n",
      "Eltus\n",
      "S\n",
      "\n",
      "\n",
      "Iteration: 23500, Loss: 22.768679\n",
      "\n",
      "Nax\n",
      "Hesptetolypsilophemasua\n",
      "Dia\n",
      "Axdasongmeruthanius\n",
      "Tstisaurus\n",
      "Aceratops\n",
      "Sakerctiouskuronvedinus\n",
      "\n",
      "\n",
      "Iteration: 24000, Loss: 22.754615\n",
      "\n",
      "Koneosaurus\n",
      "Indrosaurus\n",
      "Antrosaurus\n",
      "Dorlosaurus\n",
      "Mecosaurus\n",
      "Leptentouslnthong\n",
      "Hulobuttojesaurus\n",
      "\n",
      "\n",
      "Iteration: 24500, Loss: 22.740511\n",
      "\n",
      "Godyon\n",
      "Orocosaurus\n",
      "Beugnaterasaurus\n",
      "Elops\n",
      "Mocerasubiuches\n",
      "Lopanesaurus\n",
      "Lotom\n",
      "\n",
      "\n",
      "Iteration: 25000, Loss: 22.681791\n",
      "\n",
      "Ocermatotaurus\n",
      "Carjasourus\n",
      "Leriasaurus\n",
      "Yleisaurus\n",
      "Ekepanngysaurus\n",
      "Huechacceragus\n",
      "Losaurus\n",
      "\n",
      "\n",
      "Iteration: 25500, Loss: 22.676212\n",
      "\n",
      "Brykadr\n",
      "Bisosaurus\n",
      "Ryanachadi\n",
      "Murosaurus\n",
      "Rutahdrataus\n",
      "Androstes\n",
      "Atrobratrornathargosaurus\n",
      "\n",
      "\n",
      "Iteration: 26000, Loss: 22.784047\n",
      "\n",
      "Diceratopinasaurus\n",
      "Bisaurus\n",
      "Amagratisaurus\n",
      "Amaltoraptus\n",
      "Hisaurus\n",
      "Atatoraptor\n",
      "Lisaurus\n",
      "\n",
      "\n",
      "Iteration: 26500, Loss: 22.758715\n",
      "\n",
      "Heliypalanasaurus\n",
      "Seyllosaurus\n",
      "Respeesaurus\n",
      "Ovinus\n",
      "Istinosaurus\n",
      "Acovosiucaepalrosaurus\n",
      "Talbinsaurus\n",
      "\n",
      "\n",
      "Iteration: 27000, Loss: 22.783504\n",
      "\n",
      "Ecerophosaurus\n",
      "Tqeieoneus\n",
      "Hedoruphosaurus\n",
      "Hianosaurus\n",
      "Rotiobaphoa\n",
      "Topulaphus\n",
      "Gigaliacoptor\n",
      "\n",
      "\n",
      "Iteration: 27500, Loss: 22.854035\n",
      "\n",
      "Chelthosaurus\n",
      "Stanteiatarilim\n",
      "Atinypaflanisaurus\n",
      "Steliasaurus\n",
      "Batellasaurus\n",
      "Horia\n",
      "Hinhynolosaurus\n",
      "\n",
      "\n",
      "Iteration: 28000, Loss: 22.702928\n",
      "\n",
      "Kenuanosaurus\n",
      "Ton\n",
      "Husunophor\n",
      "Ronggornithon\n",
      "Hyidon\n",
      "Gostyongmongcepsaurus\n",
      "Gngosaurus\n",
      "\n",
      "\n",
      "Iteration: 28500, Loss: 22.649941\n",
      "\n",
      "Ngzhousaurus\n",
      "Hicepalceryongaxlopenen\n",
      "Laiepenushlbenisaurus\n",
      "Enhosaurus\n",
      "Ati\n",
      "Homosaurus\n",
      "Eumiropane\n",
      "\n",
      "\n",
      "Iteration: 29000, Loss: 22.634277\n",
      "\n",
      "Beposaurus\n",
      "Kjilopelotorauchusepsisaurus\n",
      "Hachalonel\n",
      "Kopsoliphongomaxusiuintius\n",
      "Xashuanonorititornithosaurus\n",
      "Hucochospus\n",
      "Stagus\n",
      "\n",
      "\n",
      "Iteration: 29500, Loss: 22.512729\n",
      "\n",
      "Luatansaurus\n",
      "Murisaurus\n",
      "Hucholokopnosaurus\n",
      "Rotaurus\n",
      "Acans\n",
      "Asausaurus\n",
      "Pathuelosaurus\n",
      "\n",
      "\n",
      "Iteration: 30000, Loss: 22.606232\n",
      "\n",
      "Uruiakipsosaurus\n",
      "Stadinasuril\n",
      "Apelgus\n",
      "Comuanteratops\n",
      "Liaurydon\n",
      "Strtys\n",
      "Huausaurus\n",
      "\n",
      "\n",
      "Iteration: 30500, Loss: 22.618093\n",
      "\n",
      "Humalons\n",
      "Loosaurus\n",
      "Adanaldophus\n",
      "Byaglosaurus\n",
      "Saliprrosaurus\n",
      "Syryonthongasatra\n",
      "Cenyonl\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 31000, Loss: 22.462700\n",
      "\n",
      "Belaranitan\n",
      "Verangasaurus\n",
      "Uarmia\n",
      "Diteqatops\n",
      "Ricelalosaurus\n",
      "Tescosaurus\n",
      "Abrosaurus\n",
      "\n",
      "\n",
      "Iteration: 31500, Loss: 22.392385\n",
      "\n",
      "Weciaeavosaurus\n",
      "Abpalong\n",
      "Iumosaurus\n",
      "Adiliasaurus\n",
      "Hitioradosaurus\n",
      "Cancrtha\n",
      "Criidon\n",
      "\n",
      "\n",
      "Iteration: 32000, Loss: 22.521584\n",
      "\n",
      "Phynosaurus\n",
      "Apathustanewhansaurus\n",
      "Opelta\n",
      "Seirasaurus\n",
      "Machilosaurosaurus\n",
      "Rastegonsaurus\n",
      "Paodon\n",
      "\n",
      "\n",
      "Iteration: 32500, Loss: 22.403131\n",
      "\n",
      "Celopirogsaurus\n",
      "Renochisaurus\n",
      "Gaeiconyxeosaurus\n",
      "Ylnykosaurus\n",
      "Deriiops\n",
      "Kelomagotor\n",
      "Saus\n",
      "\n",
      "\n",
      "Iteration: 33000, Loss: 22.404179\n",
      "\n",
      "Igaporosaurus\n",
      "Anousaurus\n",
      "Amsoxlesaurus\n",
      "Ropinnosaurus\n",
      "Coplkbisymimpicerovennasaurushyphyposaurus\n",
      "Burtiseus\n",
      "Canpodon\n",
      "\n",
      "\n",
      "Iteration: 33500, Loss: 22.530037\n",
      "\n",
      "Entanoceranosaurus\n",
      "Wenydratisaurus\n",
      "Iclopudameodiptor\n",
      "Rgpanosaurus\n",
      "Ryinatesaurus\n",
      "Tanchinospurosaurus\n",
      "Anati\n",
      "\n",
      "\n",
      "Iteration: 34000, Loss: 22.423785\n",
      "\n",
      "Hulonatops\n",
      "Ertellegnogalips\n",
      "Ikathostacalagosaurus\n",
      "Toranodudopaceratoratous\n",
      "Saloromalliteomi\n",
      "Liangactaronliroma\n",
      "Kicelatops\n",
      "\n",
      "\n",
      "Iteration: 34500, Loss: 22.392548\n",
      "\n",
      "Membosaurus\n",
      "Ggythosaurus\n",
      "Aushioceratops\n",
      "Oraptor\n",
      "Hacholugnosaurus\n",
      "Dastilongs\n",
      "Apsorace\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = model(data, ix_to_char, char_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
